{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified implementation of word2vec. It is missing negative sampling and it is only run on a toy dataset. \n",
    "\n",
    "The main idea behind this notebook was to get experience with Pytorch and in visualising and debugging an algorithm. \n",
    "\n",
    "Enhancements for next time: \n",
    "* work out how to make this algorithm in batches \n",
    "* try on a real dataset\n",
    "* GloVe implementation\n",
    "* something to handle unknown words, or words outside the top n \n",
    "* profile the code: where are the bottlenecks?\n",
    "* GPU integration\n",
    "\n",
    "Resources used to write this: \n",
    "* https://github.com/bollu/bollu.github.io#everything-you-know-about-word2vec-is-wrong\n",
    "* https://www.youtube.com/watch?v=kEMJRjEdNzM&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7fc282d2f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torchtest\n",
    "import pandas as pd, numpy as np, datetime, scipy.misc\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from IPython.core.debugger import set_trace\n",
    "import plotly_express as px\n",
    "import umap \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "path_results = './results/word2vec/'\n",
    "path_runs = path_results + 'runs/'\n",
    "path_data = './data/'\n",
    "torch.manual_seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     1,
     8,
     17,
     22,
     38,
     45
    ]
   },
   "outputs": [],
   "source": [
    "class SentenceData(Dataset): \n",
    "    def __init__(self, m): \n",
    "        self.m = m \n",
    "        self.corpus = self._load_train()\n",
    "        self.X = self._sen2tkn(self.corpus)\n",
    "        self.words,self.n_words  = list(set(self.X)),len(list(set(self.X)))\n",
    "        self.word2idx,self.idx2word = self._get_word_idx_mappings()\n",
    "\n",
    "    def _sen2tkn(self, l):\n",
    "        \"\"\"Convert list of sentences l into a concatenated list of tokens, adding <SOL> \n",
    "        and <EOL> tokens as needed.\"\"\"\n",
    "        def add_line_tokens(x):  return '<SOL> ' + x + ' <EOL>'\n",
    "        X_l = [add_line_tokens(o) + ' ' if i != (len(l)-1) \n",
    "                        else add_line_tokens(o) for i,o in enumerate(l)]\n",
    "        X = ''.join(X_l).split(' ')\n",
    "        return X\n",
    "\n",
    "    def _load_train(self): \n",
    "        ## read sentences, return list with one sentence per line\n",
    "        corpus = open(path_data + 'simple_sentences.txt').read().split('\\n')\n",
    "        return corpus\n",
    "\n",
    "    def _get_word_idx_mappings(self): \n",
    "        word2idx,idx2word = dict(),dict()\n",
    "        for i,o in enumerate(self.words): word2idx[o] = i; idx2word[i] = o\n",
    "        return word2idx,idx2word\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        if type(idx) is not slice:     idx = slice(idx,idx+1)\n",
    "        tmp = self.corpus[idx]\n",
    "        # convert to index \n",
    "        x = [self.word2idx[o] for o in self._sen2tkn(tmp)]\n",
    "        return x\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "m = 2\n",
    "train_ds = SentenceData(m=m)\n",
    "def collate_fn(x):\n",
    "    \"\"\"Flatten out list of lists\"\"\"\n",
    "    l = []\n",
    "    for o in x: l += o\n",
    "    return l\n",
    "# Batch size has become how many sentences to include in one row \n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "code_folding": [
     2,
     34
    ]
   },
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module): \n",
    "    def __init__(self, m, emb_sz, ds):\n",
    "        \"\"\"If a layer has trainable parameters, it is defined in init. \n",
    "        ds = dataset\"\"\"\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.m,self.emb_sz,self.ds = m,emb_sz,ds\n",
    "        # U holds context words, V holds center words\n",
    "        self.U,self.V = self._init_embedding(),self._init_embedding()  \n",
    "        \n",
    "    def _init_embedding(self): \n",
    "        return nn.Parameter(torch.randn((self.emb_sz, self.ds.n_words)) *\\\n",
    "                            np.sqrt(1/self.ds.n_words))\n",
    "    \n",
    "    def forward(self, x, input_independent=False): \n",
    "        \"\"\" Returns tensor of log probabilities\n",
    "        x is a tensor of some sort\"\"\" \n",
    "        logp_l = []\n",
    "        if input_independent: x = [0 for o in x]\n",
    "        for i, c in enumerate(x):   # enumerates for center word c \n",
    "            dotprods = self._get_dotprods(c) # use in calculating p below\n",
    "            min_idx,max_idx = max(0,i-self.m), min(len(x)-1,i+self.m)\n",
    "            for j in range(min_idx, max_idx+1):   # enumerate context words \n",
    "                if j != i:    # don't eval the center word\n",
    "                    logp = self.predict(c, o=x[j], dotprods=dotprods)\n",
    "                    logp_l.append(logp)\n",
    "        return torch.stack(logp_l)\n",
    "    \n",
    "    def _get_dotprods(self, c): \n",
    "        v,U = self.V[:,c].reshape(-1,1),self.U\n",
    "        dotprods = (v * U).sum(0)\n",
    "        return dotprods\n",
    "    \n",
    "    def predict(self, c, o, dotprods=None):\n",
    "        \"\"\"\n",
    "        Takes current values of U and V and predicts log probability of context word o \n",
    "        appearing in context window of center word c \n",
    "        c: index of center word \n",
    "        o: index of context word\n",
    "        dotprods: saves computation if you pass those in\"\"\"\n",
    "        if dotprods is None: dotprods = self._get_dotprods(c)\n",
    "        return F.log_softmax(dotprods, dim=0)[o]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "code_folding": [
     2,
     9
    ]
   },
   "outputs": [],
   "source": [
    "# I wasn't quite sure where to put this code. \n",
    "\n",
    "def add_run_info_to_tensorboard(hparam_d, metric_d, model): \n",
    "    writer.add_hparams(hparam_d, metric_d)\n",
    "    # Umap embeddings\n",
    "    emb = ((model.V+model.U) /2).T\n",
    "    writer.add_embedding(emb,metadata=model.ds.words,\n",
    "                     tag='final_embeddings')\n",
    "\n",
    "def update_tensorboard(global_step, loss, model, predict_sentences=False):\n",
    "    # loss, weights, grads\n",
    "    writer.add_scalar(tag='training_loss', scalar_value=loss, \n",
    "                          global_step=global_step)\n",
    "    writer.add_histogram(tag='V_weights', values=model.V, global_step=global_step)\n",
    "    writer.add_histogram(tag='U_weights', values=model.U, global_step=global_step)\n",
    "    writer.add_histogram(tag='V_grad', values=model.V.grad, global_step=global_step)\n",
    "    writer.add_histogram(tag='U_grad', values=model.U.grad, global_step=global_step)\n",
    "    if predict_sentences:\n",
    "        word_pairs = [('dog','cute'), ('cute', 'dog'), ('dog','dog'), ('<EOL>','<SOL>'),\n",
    "                      ('bank','money'), ('withdraw','money'), ('money','withdraw'),\n",
    "                      ('dog','money'), ('bank','cat')]\n",
    "        pred_d = dict()\n",
    "        for c, o in word_pairs: \n",
    "            key = \"p_\" + c + \"_\" + o\n",
    "            c_idx,o_idx = model.ds.word2idx[c],model.ds.word2idx[o]\n",
    "            pred_d[key] = torch.exp(model.predict(c_idx, o_idx)).item()\n",
    "        writer.add_scalars('example_predictions', pred_d, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.749580747202824\n",
      "1 2.619847034153185\n",
      "2 2.495410216482062\n",
      "3 2.3872072822169255\n",
      "4 2.347351036573711\n",
      "5 2.2759331778476115\n",
      "6 2.2536559606853284\n",
      "7 2.242249325702065\n",
      "8 2.2260925393355517\n",
      "9 2.2234162154950594\n",
      "10 2.216376097578751\n",
      "11 2.2052205612784936\n",
      "12 2.211290767318324\n",
      "13 2.205070834410818\n",
      "14 2.1938008509184184\n",
      "15 2.1901651934573527\n",
      "16 2.186603596335963\n",
      "17 2.182089767957989\n",
      "18 2.169069327806172\n",
      "19 2.1922519583451114\n",
      "20 2.1808404985227083\n",
      "21 2.188620385370757\n",
      "22 2.1638218352669165\n",
      "23 2.1752220956902755\n",
      "24 2.1752996319218685\n",
      "25 2.172049610238327\n",
      "26 2.1792183800747527\n",
      "27 2.1476881692284033\n",
      "28 2.1664692226209135\n",
      "29 2.1794854590767305\n",
      "30 2.1572324476744003\n",
      "31 2.1705891395870007\n",
      "32 2.1804998673890768\n",
      "33 2.1665854077590145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-60c8c67aeabb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mthisloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss_movingavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthisloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprintfreq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Run model \n",
    "m = 2  # number of context words\n",
    "emb_sz = 15  # number of embedding dimensions\n",
    "model = Word2Vec(m, emb_sz, train_ds)\n",
    "\n",
    "## the word2vec algorithm with SGD \n",
    "# with SGDa\n",
    "lr = 0.01\n",
    "n_epochs = 400\n",
    "printfreq = 1\n",
    "batch_sz = 100\n",
    "idx = 0 \n",
    "epoch = 0 \n",
    "log_dir = path_runs + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = SummaryWriter(log_dir = log_dir)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "def loss_fn(logp_l):    return -torch.mean(logp_l)\n",
    "loss_movingavg = []     # list to work out moving average of loss\n",
    "for epoch in range(n_epochs): \n",
    "    thisloss = 0\n",
    "    for i,x in enumerate(train_dl): \n",
    "        opt.zero_grad()\n",
    "        logp_l = model(x)  # log preds \n",
    "        loss = loss_fn(logp_l)\n",
    "        thisloss += loss.item() / len(train_dl)\n",
    "        loss_movingavg.append(thisloss)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if epoch % printfreq == 0:\n",
    "        global_step = epoch * len(train_dl)\n",
    "        loss = thisloss\n",
    "        update_tensorboard(global_step, loss, model, predict_sentences=True)\n",
    "        print(epoch, thisloss )\n",
    "        loss_movingavg = []\n",
    "\n",
    "# Hparams\n",
    "hparam_d = {'emb_sz': emb_sz, 'm': m, 'lr': lr, 'batch_sz':batch_sz,\n",
    "            'n_epochs':n_epochs, 'printfreq': printfreq}\n",
    "metric_d = {'hparam/loss': np.mean(loss_movingavg)}   \n",
    "add_run_info_to_tensorboard(hparam_d, metric_d, model)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class are methods to check statistics of the input, visualise the input, get loss of some simple baselines, check the variables in the model actually update,  visualise the output via UMAP, create distance matrices between the output embeddings, find most similar and dissimilar words to a given word, and so on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "code_folding": [
     1,
     10,
     32,
     38,
     81,
     84,
     88,
     108,
     114,
     125,
     143
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingDiagnostics: \n",
    "    def __init__(self, model, dl, opt,loss_fn, seed=420): \n",
    "        \"\"\"dl: data_loader\"\"\"        \n",
    "        torch.manual_seed(seed)\n",
    "        self.model,self.dl,self.opt,self.loss_fn = model,dl,opt,loss_fn\n",
    "        # We average together V and U vectors to get the final embedding \n",
    "        self.emb = ((self.model.U + self.model.V) / 2).detach().numpy()\n",
    "        self.df_emb = pd.DataFrame(self.emb, columns =  self.model.ds.words)\n",
    "        self.df_emb_normalised = self._normalise_embedding_df(self.df_emb)\n",
    "             \n",
    "    def check_variables_update(self): \n",
    "        \"\"\"\n",
    "        This checks that parameters are being updated. \n",
    "        We run one forward pass+backward pass, and then update the parameters once, \n",
    "        and look at what changed. \n",
    "        \"\"\"\n",
    "        x = next(iter(self.dl))  # get one batch from data loader \n",
    "        params = [o for o in self.model.named_parameters() if o[1].requires_grad]\n",
    "        # copy initial values\n",
    "        initial_params = [(name, p.clone()) for (name, p) in params]\n",
    "        print(\"---- Parameters with 'requires_grad' and their sizes ------\")\n",
    "        for (name, p) in initial_params:  print(name, p.size())\n",
    "        # take a step\n",
    "        self.opt.zero_grad()\n",
    "        logp_l = self.model(x)\n",
    "        loss = self.loss_fn(logp_l)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        print(\"---- Matrix norm of parameter update for one step ------\")\n",
    "        for (_,old_p), (name, new_p) in zip(initial_params, params): \n",
    "            print (name, torch.norm(new_p - old_p).item())       \n",
    "            \n",
    "    def check_inputs(self): \n",
    "        \"\"\"Select some inputs, translate them, print them\"\"\"\n",
    "        idxs = self.model.ds[1:4]\n",
    "        print(idxs)\n",
    "        print([ds.idx2word[o] for o in idxs])\n",
    "    \n",
    "    def get_baseline_loss(self, baseline='random'): \n",
    "        \"\"\"\n",
    "        Returns baseline loss to compare our model against. \n",
    "        These let us know how good or bad we are doing.  \n",
    "        \n",
    "        We run through one pass of the data, assuming that we predict each\n",
    "               word with equal probability. \n",
    "        Params\n",
    "            baseline: \n",
    "                \"random\": p(word) = 1/n_words, i.e. a uniform dist\n",
    "                \"input_independent\": replace all sentences with all one character\n",
    "                \"bag_of_words\": p(word) = p(word|bag of words(train_set)) \n",
    "                \"coocurrences\": p(word) = p(word|center word c, distance m, train set)\n",
    "                                Basically looking at word cofrequencies to get probability.\n",
    "                                I think this is the best loss possible for our model.   \n",
    "        \"\"\"\n",
    "        baseline_vals = [\"random\", \"bag_of_words\", \"coocurrences\"]\n",
    "        if baseline not in baseline_vals:   raise Exception('Baseline keyword not recognised')  \n",
    "        logp_l = []\n",
    "        if baseline == \"random\": return torch.tensor(-np.log(1/self.model.ds.n_words))\n",
    "        if baseline == \"bag_of_words\": \n",
    "            freqs = self.get_word_input_counts() / sum(self.get_word_input_counts())\n",
    "            freqs.index = [self.model.ds.word2idx[o] for o in freqs.index]\n",
    "            idx2freq = dict(freqs)\n",
    "        if baseline == 'coocurrences':\n",
    "            counts = self.get_word_cooccurences()\n",
    "            prob_mat = (counts / counts.values.sum(axis=1))\n",
    "        for x in self.dl:  # one epoch, going through all examples\n",
    "            for i, c in enumerate(x):  # enumerate center words \n",
    "                min_idx = max(0,          i - self.model.m)\n",
    "                max_idx = min(len(x) - 1, i + self.model.m)\n",
    "                for j in range(min_idx, max_idx + 1):   # enumerate context words \n",
    "                    if j != i:  # don't eval the center word\n",
    "                        if baseline == \"bag_of_words\":    \n",
    "                            logp = torch.log(torch.tensor(idx2freq[x[j]]))\n",
    "                        elif baseline == \"coocurrences\":\n",
    "                            logp = torch.log(torch.tensor(\n",
    "                                prob_mat.iloc[:,c].loc[self.model.ds.idx2word[x[j]]])) \n",
    "                        else: \n",
    "                            logp = self.model.predict(c, o=x[j])\n",
    "                        logp_l.append(logp)\n",
    "        return loss_fn(torch.stack(logp_l))\n",
    "\n",
    "    def _normalise_embedding_df(self, df):  \n",
    "        return df /np.linalg.norm(df.values,ord=2,axis=0,keepdims=True)\n",
    "    \n",
    "    def get_word_input_counts(self):   \n",
    "        \"\"\"Count how often each word appears in input\"\"\"\n",
    "        return pd.Series(model.ds.X).value_counts()\n",
    "\n",
    "    def get_word_cooccurences(self): \n",
    "        \"\"\"\n",
    "        Count how often a word appears within distance m of another word \n",
    "        Returns a symmetrical data frame, where the column is the center word, \n",
    "         and the row is how often that word  appears in the m-window \n",
    "         around the center word\n",
    "        \"\"\" \n",
    "        m,n_words,X = self.model.m,self.model.ds.n_words,self.model.ds.X\n",
    "        words,word2idx = self.model.ds.words,self.model.ds.word2idx\n",
    "        count_df = pd.DataFrame(np.zeros((n_words,n_words)),dtype=int,\n",
    "                               index = words, columns = words)\n",
    "        # X is the list of words\n",
    "        for i,c in enumerate(X): \n",
    "            min_idx,max_idx = max(0,i-m),min(len(X)-1,i+m)\n",
    "            for j in range(min_idx,max_idx+1): \n",
    "                if j == i: continue\n",
    "                o_idx = word2idx[X[j]]\n",
    "                count_df.iloc[o_idx][word2idx[c]] += 1\n",
    "        return count_df\n",
    "    \n",
    "    def _get_dist_mat(self, df, metric): \n",
    "        \"\"\"df: pandas data frame; metric: anything fitting in pairwise distances\"\"\"\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        return pd.DataFrame(pairwise_distances(df.values.T, metric=metric).round(3), \n",
    "                           columns = self.df_emb.columns, index = self.df_emb.columns)\n",
    "    \n",
    "    def get_similar_n_words(self, w, n=5, direction='closest', metric='cosine'): \n",
    "        \"\"\"\n",
    "        Given a word w, what word is closest/furthest to it? \n",
    "            w: word, emb: embeddings as np array\n",
    "            direction: one of closest or furthest\"\"\"\n",
    "        dist_mat = self._get_dist_mat(self.df_emb, metric=metric)\n",
    "        v = dist_mat[w]\n",
    "        if    direction == 'closest' : return v.nsmallest(n+1)[1:]\n",
    "        elif  direction == 'furthest': return v.nlargest(n)\n",
    "        else: raise Exception(\"direction must be one of 'closest' or 'furthest'\")\n",
    "             \n",
    "    def plot_umap(self, n_dims=3, n_neighbours=3): \n",
    "        \"\"\"\n",
    "        Perform dim reduction with umap and plot results\n",
    "        n_dims: number of output dimensions \n",
    "        n_neighbours: local/global parameter for UMAP\"\"\"\n",
    "        if   n_dims == 2:   cols = ['dim1','dim2']\n",
    "        elif n_dims == 3:   cols = ['dim1','dim2','dim3']\n",
    "        else:               raise Exception('dims should be 2 or 3')\n",
    "        # put your data values in the fit_transform bit \n",
    "        emb_umap = umap.UMAP(n_neighbors=n_neighbours, n_components=n_dims).fit_transform(self.emb.T)\n",
    "        # put in pandas dataframe to help plotting with plotly express\n",
    "        emb_umap_df = pd.DataFrame(emb_umap, columns = cols)\n",
    "        emb_umap_df['word'] = self.model.ds.words\n",
    "        if n_dims == 2: \n",
    "            return px.scatter(emb_umap_df,    x='dim1', y='dim2',          hover_name='word')\n",
    "        elif n_dims ==3:\n",
    "            return px.scatter_3d(emb_umap_df, x='dim1', y='dim2',z='dim3', hover_name='word')\n",
    "\n",
    "    def write2file(self, path_results): \n",
    "        \"\"\"Write some distance matrices and embedding matrices to file to inspect manually\"\"\"\n",
    "        df_U = pd.DataFrame(self.model.U.detach().numpy())\n",
    "        df_V = pd.DataFrame(self.model.V.detach().numpy())\n",
    "        df_U.columns = df_V.columns = self.df_emb.columns\n",
    "\n",
    "        # distance matrices \n",
    "        df_emb_cosine = self._get_dist_mat(self.df_emb,'cosine')\n",
    "        df_emb_l2     = self._get_dist_mat(self.df_emb,'euclidean')\n",
    "        df_emb_cosine.columns = df_emb_l2.columns = self.df_emb.columns\n",
    "        df_emb_cosine.index   = df_emb_l2.index   = self.df_emb.columns\n",
    "\n",
    "        with pd.ExcelWriter(path_results + 'word2vec_vectors.xlsx') as writer: \n",
    "            df_V.to_excel(         writer, sheet_name='center_words'          )\n",
    "            df_U.to_excel(         writer, sheet_name='context_words'         )  \n",
    "            self.df_emb.to_excel(  writer, sheet_name='emb_words'             )\n",
    "            df_emb_cosine.to_excel(writer, sheet_name='emb_cosine_distance'   )\n",
    "            df_emb_l2.to_excel(    writer, sheet_name='emb_euclidean_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "d = EmbeddingDiagnostics(model, train_dl, opt, loss_fn)\n",
    "#d.get_word_input_counts()\n",
    "#d.get_word_cooccurences()\n",
    "#d.get_similar_n_words('<SOL>',direction = 'closest', metric='euclidean')\n",
    "#d.write2file(path_results)\n",
    "#d.plot_umap(n_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 11, 14, 11, 12, 13, 15, 14, 11, 3, 13, 15, 4, 11, 3, 11, 14, 11, 13]\n",
      "['<SOL>', 'dog', 'fluffy', 'dog', 'fun', '<EOL>', '<SOL>', 'fluffy', 'dog', 'small', '<EOL>', '<SOL>', 'cute', 'dog', 'small', 'dog', 'fluffy', 'dog', '<EOL>']\n"
     ]
    }
   ],
   "source": [
    "#d.get_baseline_loss()\n",
    "#d.check_variables_update()    \n",
    "# d.check_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
