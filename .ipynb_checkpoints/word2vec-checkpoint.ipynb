{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used to write this: \n",
    "* https://github.com/bollu/bollu.github.io#everything-you-know-about-word2vec-is-wrong\n",
    "* https://www.youtube.com/watch?v=kEMJRjEdNzM&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe35112d2f0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torchtest\n",
    "import pandas as pd, numpy as np, datetime, scipy.misc\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from IPython.core.debugger import set_trace\n",
    "import plotly_express as px\n",
    "import umap \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "path_results = './results/word2vec/'\n",
    "path_runs = path_results + 'runs/'\n",
    "path_data = './data/'\n",
    "torch.manual_seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some thoughts on where to go from here: \n",
    "* ~~revamp data with the Dataset and DataLoader architecture~~ \n",
    "* ~~put algorithm into a class that calls torch.nn~~\n",
    "* ~~simplify code into functions~~\n",
    "* ~~shuffle sentences on the training set~~ \n",
    "* some kind of test set testing \n",
    "* work out how to make this algorithm in batches \n",
    "* adjust scaling of the starting parameters\n",
    "* ~~do an average loss function over a few batches rather than one~~\n",
    "* put on GPU somehow (UTS? Colab?) \n",
    "* try on a real dataset\n",
    "* GloVe\n",
    "* something to handle unknown words, or words outside the top n \n",
    "* train/test split for dataset\n",
    "* where can you integrate tensorboard code?\n",
    "* where do you put tests?\n",
    "\n",
    "Tensorboard\n",
    "* ~~hook it up to TensorBoard and print out some training graphs in real time~~\n",
    "* ~~visualise the gradients and the weights of the model~~\n",
    "* plot performance on a holdout set over time (??)\n",
    "* ~~log hyperparameters (m,lr, batch_sz)~~\n",
    "* ~~plot embeddings in tensorboard~~\n",
    "\n",
    "* profile the code: where are the bottlenecks?\n",
    "* ~~put this on github~~\n",
    "* manually calculate the gradients with finite differences and compare them to the autograd version\n",
    "* intuitively understand the softmax part of this. what's the motivation, what are strengths and weaknesses going to be\n",
    "* try out Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     1,
     7,
     16,
     21,
     26,
     32
    ]
   },
   "outputs": [],
   "source": [
    "class SentenceData(Dataset): \n",
    "    def __init__(self): \n",
    "        self.corpus = self._load_train()\n",
    "        self.X = self._sen2tkn(self.corpus)\n",
    "        self.words,self.n_words  = list(set(self.X)),len(list(set(self.X)))\n",
    "        self.word2idx,self.idx2word = self._get_word_idx_mappings()\n",
    "\n",
    "    def _sen2tkn(self, l):\n",
    "        \"\"\"Convert list of sentences l into a concatenated list of tokens, adding <SOL> \n",
    "        and <EOL> tokens as needed.\"\"\"\n",
    "        def add_line_tokens(x):  return '<SOL> ' + x + ' <EOL>'\n",
    "        X_l = [add_line_tokens(o) + ' ' if i != (len(l)-1) \n",
    "                        else add_line_tokens(o) for i,o in enumerate(l)]\n",
    "        X = ''.join(X_l).split(' ')\n",
    "        return X\n",
    "\n",
    "    def _load_train(self): \n",
    "        ## read sentences, return list with one sentence per line\n",
    "        corpus = open(path_data + 'simple_sentences.txt').read().split('\\n')\n",
    "        return corpus\n",
    "\n",
    "    def _get_word_idx_mappings(self): \n",
    "        word2idx,idx2word = dict(),dict()\n",
    "        for i,o in enumerate(self.words): word2idx[o] = i; idx2word[i] = o\n",
    "        return word2idx,idx2word\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        if type(idx) is not slice:     idx = slice(idx,idx+1)\n",
    "        tmp = self.corpus[idx]\n",
    "        # convert to index \n",
    "        return [self.word2idx[o] for o in self._sen2tkn(tmp)]\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "train_ds = SentenceData()\n",
    "def collate_fn(x):\n",
    "    \"\"\"Flatten out list of lists\"\"\"\n",
    "    l = []\n",
    "    for o in x: l += o\n",
    "    return l\n",
    "# Batch size has become how many sentences to include in one row \n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     3,
     11,
     14
    ]
   },
   "outputs": [],
   "source": [
    "# For now, we do the preprocessing in the SentenceData class, \n",
    "# and we assume that processed data is passed to this class. \n",
    "class Word2Vec(nn.Module): \n",
    "    def __init__(self, m, emb_sz, ds):\n",
    "        \"\"\"If a layer has trainable parameters, it is defined in init. \n",
    "        ds = dataset\"\"\"\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.m,self.emb_sz,self.ds = m,emb_sz,ds\n",
    "        # U holds context words, V holds center words\n",
    "        self.U,self.V = self._init_embedding(),self._init_embedding()  \n",
    "        \n",
    "    def _init_embedding(self): \n",
    "        return nn.Parameter(torch.rand((self.emb_sz, self.ds.n_words))*1)\n",
    "    \n",
    "    def _softmax(self, x): \n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        z = x - max(x)\n",
    "        return torch.exp(z) / torch.sum(torch.exp(z))\n",
    "    \n",
    "    def forward(self, x): \n",
    "        \"\"\"x is a tensor of some sort\"\"\" \n",
    "        loss_sum = 0\n",
    "        z = 0  # counts how many losses we add up (required for loss_sum)\n",
    "        for i, c in enumerate(x):   # enumerates for center word c \n",
    "            # use in calculating p below\n",
    "            dotprods = (self.V[:,c].reshape(-1,1)*self.U).sum(0)  \n",
    "            min_idx,max_idx = max(0,i-self.m), min(len(x)-1,i+self.m)\n",
    "            for j in range(min_idx, max_idx+1):   # enumerate context words \n",
    "                if j == i: continue   # don't eval the center word\n",
    "                p = self._softmax(dotprods)[x[j]]\n",
    "                z +=1\n",
    "                loss_sum += torch.log(p)\n",
    "        loss_sum = -loss_sum / z\n",
    "        return loss_sum\n",
    "    \n",
    "    def predict(self, c, o):\n",
    "        \"\"\"Takes current values of U and V and predicts the probability of context word o \n",
    "        appearing in context window of center word c \"\"\"\n",
    "        c_idx,o_idx = self.ds.word2idx[c],self.ds.word2idx[o]\n",
    "        v = self.V[:,c_idx].reshape(-1,1)\n",
    "        dotprods = (v*self.U).sum(0)\n",
    "        return self._softmax(dotprods)[o_idx].item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def add_run_info_to_tensorboard(hparam_d, metric_d, model): \n",
    "    writer.add_hparams(hparam_d, metric_d)\n",
    "    # Umap embeddings\n",
    "    emb = ((model.V+model.U) /2).T\n",
    "    writer.add_embedding(emb,metadata=model.ds.words,\n",
    "                     tag='final_embeddings')\n",
    "\n",
    "def update_tensorboard(global_step, loss, model, predict_sentences=False):\n",
    "    # loss, weights, grads\n",
    "    writer.add_scalar(tag='training_loss', scalar_value=loss, \n",
    "                          global_step=global_step)\n",
    "    writer.add_histogram(tag='V_weights', values=model.V, global_step=global_step)\n",
    "    writer.add_histogram(tag='U_weights', values=model.U, global_step=global_step)\n",
    "    writer.add_histogram(tag='V_grad', values=model.V.grad, global_step=global_step)\n",
    "    writer.add_histogram(tag='U_grad', values=model.U.grad, global_step=global_step)\n",
    "    if predict_sentences:\n",
    "        # some example predictions\n",
    "        preds_d = {\n",
    "                'p_dog_cute':          model.predict('dog','cute'),\n",
    "                'p_cute_dog':          model.predict('cute', 'dog'),\n",
    "                'p_dog_dog':           model.predict('dog','dog'),\n",
    "                'p_eol_sol':           model.predict('<EOL>','<SOL>'),\n",
    "                'p_bank_money':        model.predict('bank','money'),\n",
    "                'p_withdraw_money':    model.predict('withdraw','money'),\n",
    "                'p_money_withdraw':    model.predict('money','withdraw'),\n",
    "                \"p_dog_money\":         model.predict('dog','money'),\n",
    "                \"p_bank_cat\":          model.predict('bank','cat')\n",
    "            }\n",
    "        writer.add_scalars('example_predictions', preds_d, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.727652800710578\n",
      "50 2.1705847664883264\n",
      "100 2.1590568642867245\n",
      "150 2.16153799860101\n",
      "200 2.157130379425852\n",
      "250 2.1623246481544096\n",
      "300 2.147075251529091\n",
      "350 2.1446270879946256\n"
     ]
    }
   ],
   "source": [
    "### Run model \n",
    "\n",
    "m = 2  # number of context words\n",
    "emb_sz = 15  # number of embedding dimensions\n",
    "model = Word2Vec(m, emb_sz, train_ds)\n",
    "\n",
    "## the word2vec algorithm with SGD \n",
    "# with SGDa\n",
    "lr = 0.01\n",
    "n_epochs = 400\n",
    "printfreq = 50\n",
    "batch_sz = 16\n",
    "idx = 0 \n",
    "epoch = 0 \n",
    "log_dir = path_runs + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = SummaryWriter(log_dir = log_dir)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_movingavg = []     # list to work out moving average of loss\n",
    "for epoch in range(n_epochs): \n",
    "    thisloss = 0\n",
    "    for i,x in enumerate(train_dl): \n",
    "        opt.zero_grad()\n",
    "        loss = model(x)\n",
    "        thisloss += loss.item() / len(train_dl)\n",
    "        loss_movingavg.append(thisloss)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if epoch % printfreq == 0:\n",
    "        global_step = epoch * len(train_dl)\n",
    "        loss = thisloss\n",
    "        update_tensorboard(global_step, loss, model, predict_sentences=True)\n",
    "        print(epoch, thisloss )\n",
    "        loss_movingavg = []\n",
    "\n",
    "# Hparams\n",
    "hparam_d = {'emb_sz': emb_sz, 'm': m, 'lr': lr, 'batch_sz':batch_sz,\n",
    "            'n_epochs':n_epochs, 'printfreq': printfreq}\n",
    "metric_d = {'hparam/loss': np.mean(loss_movingavg)}   \n",
    "add_run_info_to_tensorboard(hparam_d, metric_d, model)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure everything works as it is supposed to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check variables actually get trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Parameters with 'requires_grad' and their sizes ------\n",
      "U torch.Size([15, 16])\n",
      "V torch.Size([15, 16])\n",
      "---- Matrix norm of parameter update for one step ------\n",
      "U 0.0557783804833889\n",
      "V 0.05241953581571579\n"
     ]
    }
   ],
   "source": [
    "def check_variables_update(model, opt, x): \n",
    "    \"\"\"m: model\n",
    "    opt: optimiser\n",
    "    x: some input\"\"\"\n",
    "    params = [o for o in model.named_parameters() if o[1].requires_grad]\n",
    "    # copy initial values\n",
    "    initial_params = [ (name, p.clone()) for (name, p) in params]\n",
    "    print(\"---- Parameters with 'requires_grad' and their sizes ------\")\n",
    "    for (name, p) in initial_params:  print(name, p.size())\n",
    "    # take a step\n",
    "    opt.zero_grad()\n",
    "    loss = model(x)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"---- Matrix norm of parameter update for one step ------\")\n",
    "    for (_,old_p), (name, new_p) in zip(initial_params, params): \n",
    "        print (name, torch.norm(new_p - old_p).item())       \n",
    "check_variables_update(model, opt, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check performance against a baseline \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Uniform prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "code_folding": [
     1,
     9,
     49,
     56,
     76,
     82,
     93,
     111
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingDiagnostics: \n",
    "    def __init__(self, model, dl): \n",
    "        \"\"\"dl: data_loader\"\"\"\n",
    "        self.model,self.dl = model,dl\n",
    "        # We average together V and U vectors to get the final embedding \n",
    "        self.emb = ((self.model.U + self.model.V) / 2).detach().numpy()\n",
    "        self.df_emb = pd.DataFrame(self.emb, columns =  self.model.ds.words)\n",
    "        self.df_emb_normalised = self._normalise_embedding_df(self.df_emb)\n",
    "        \n",
    "    def get_baseline_loss(self, baseline='random'): \n",
    "        \"\"\"\n",
    "        Returns baseline loss to compare our model against. \n",
    "        If we don't get better than these losses, our model isn't learning anything. \n",
    "        \n",
    "        We run through one pass of the data, assuming that we predict each\n",
    "               word with equal probability. \n",
    "        Params\n",
    "            baseline: \n",
    "                \"random\": p(word) = 1/n_words, i.e. a uniform dist\n",
    "                \"bag_of_words\": p(word) = p(word|bag of words(train_set)) \n",
    "                \"coocurrences\": p(word) = p(word|center word c, distance m, train set)\n",
    "                                  Basically looking at word cofrequencies to get probability. \n",
    "        \"\"\"\n",
    "        z = 0  \n",
    "        loss_sum = 0\n",
    "        if baseline == \"bag_of_words\": \n",
    "            freqs = self.get_word_input_counts() / sum(self.get_word_input_counts())\n",
    "            freqs.index = [self.model.ds.word2idx[o] for o in freqs.index]\n",
    "            idx2freq = dict(freqs)\n",
    "        if baseline == 'coocurrences':\n",
    "            counts = self.get_word_cooccurences()\n",
    "            prob_mat = (counts / counts.values.sum(axis=1))\n",
    "        for x in self.dl:  # enumerate each example\n",
    "            for i, c in enumerate(x):  # enumerate center words \n",
    "                min_idx = max(0,          i - self.model.m)\n",
    "                max_idx = min(len(x) - 1, i + self.model.m)\n",
    "                for j in range(min_idx, max_idx + 1):   # enumerate context words \n",
    "                    if j == i: continue   # don't eval the center word\n",
    "                    if baseline == \"random\":            p = 1 / model.ds.n_words\n",
    "                    elif baseline == \"bag_of_words\":    p = idx2freq[x[j]]\n",
    "                    elif baseline == \"coocurrences\":\n",
    "                        p = prob_mat.iloc[:,c].loc[self.model.ds.idx2word[x[j]]]\n",
    "                    else:\n",
    "                        raise Exception('Baseline keyword not recognised')\n",
    "                    z += 1\n",
    "                    loss_sum += torch.log(torch.tensor(p))\n",
    "        loss_sum = -loss_sum / z\n",
    "        return loss_sum\n",
    "\n",
    "    def _normalise_embedding_df(self, df):  \n",
    "        return df /np.linalg.norm(df.values,ord=2,axis=0,keepdims=True)\n",
    "    \n",
    "    def get_word_input_counts(self):   \n",
    "        \"\"\"Count how often each word appears in input\"\"\"\n",
    "        return pd.Series(model.ds.X).value_counts()\n",
    "\n",
    "    def get_word_cooccurences(self): \n",
    "        \"\"\"\n",
    "        Count how often a word appears within distance m of another word \n",
    "        Returns a symmetrical data frame, where the column is the center word, \n",
    "         and the row is how often that word  appears in the m-window \n",
    "         around the center word\n",
    "        \"\"\" \n",
    "        m,n_words,X = self.model.m,self.model.ds.n_words,self.model.ds.X\n",
    "        words,word2idx = self.model.ds.words,self.model.ds.word2idx\n",
    "        count_df = pd.DataFrame(np.zeros((n_words,n_words)),dtype=int,\n",
    "                               index = words, columns = words)\n",
    "        # X is the list of words\n",
    "        for i,c in enumerate(X): \n",
    "            min_idx,max_idx = max(0,i-m),min(len(X)-1,i+m)\n",
    "            for j in range(min_idx,max_idx+1): \n",
    "                if j == i: continue\n",
    "                o_idx = word2idx[X[j]]\n",
    "                count_df.iloc[o_idx][word2idx[c]] += 1\n",
    "        return count_df\n",
    "    \n",
    "    def _get_dist_mat(self, df, metric): \n",
    "        \"\"\"df: pandas data frame; metric: anything fitting in pairwise distances\"\"\"\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        return pd.DataFrame(pairwise_distances(df.values.T, metric=metric).round(3), \n",
    "                           columns = self.df_emb.columns, index = self.df_emb.columns)\n",
    "    \n",
    "    def get_similar_n_words(self, w, n=5, direction='closest', metric='cosine'): \n",
    "        \"\"\"\n",
    "        Given a word w, what word is closest/furthest to it? \n",
    "            w: word, emb: embeddings as np array\n",
    "            direction: one of closest or furthest\"\"\"\n",
    "        dist_mat = self._get_dist_mat(self.df_emb, metric=metric)\n",
    "        v = dist_mat[w]\n",
    "        if    direction == 'closest' : return v.nsmallest(n+1)[1:]\n",
    "        elif  direction == 'furthest': return v.nlargest(n)\n",
    "        else: raise Exception(\"direction must be one of 'closest' or 'furthest'\")\n",
    "             \n",
    "    def plot_umap(self, n_dims=3, n_neighbours=3): \n",
    "        \"\"\"\n",
    "        Perform dim reduction with umap and plot results\n",
    "        n_dims: number of output dimensions \n",
    "        n_neighbours: local/global parameter for UMAP\"\"\"\n",
    "        if   n_dims == 2:   cols = ['dim1','dim2']\n",
    "        elif n_dims == 3:   cols = ['dim1','dim2','dim3']\n",
    "        else:               raise Exception('dims should be 2 or 3')\n",
    "        # put your data values in the fit_transform bit \n",
    "        emb_umap = umap.UMAP(n_neighbors=n_neighbours, n_components=n_dims).fit_transform(self.emb.T)\n",
    "        # put in pandas dataframe to help plotting with plotly express\n",
    "        emb_umap_df = pd.DataFrame(emb_umap, columns = cols)\n",
    "        emb_umap_df['word'] = self.model.ds.words\n",
    "        if n_dims == 2: \n",
    "            return px.scatter(emb_umap_df,    x='dim1', y='dim2',          hover_name='word')\n",
    "        elif n_dims ==3:\n",
    "            return px.scatter_3d(emb_umap_df, x='dim1', y='dim2',z='dim3', hover_name='word')\n",
    "\n",
    "    def write2file(self, path_results): \n",
    "        \"\"\"Write some distance matrices and embedding matrices to file to inspect manually\"\"\"\n",
    "        df_U = pd.DataFrame(self.model.U.detach().numpy())\n",
    "        df_V = pd.DataFrame(self.model.V.detach().numpy())\n",
    "        df_U.columns = df_V.columns = self.df_emb.columns\n",
    "\n",
    "        # distance matrices \n",
    "        df_emb_cosine = self._get_dist_mat(self.df_emb,'cosine')\n",
    "        df_emb_l2     = self._get_dist_mat(self.df_emb,'euclidean')\n",
    "        df_emb_cosine.columns = df_emb_l2.columns = self.df_emb.columns\n",
    "        df_emb_cosine.index   = df_emb_l2.index   = self.df_emb.columns\n",
    "\n",
    "        with pd.ExcelWriter(path_results + 'word2vec_vectors.xlsx') as writer: \n",
    "            df_V.to_excel(         writer, sheet_name='center_words'          )\n",
    "            df_U.to_excel(         writer, sheet_name='context_words'         )  \n",
    "            self.df_emb.to_excel(  writer, sheet_name='emb_words'             )\n",
    "            df_emb_cosine.to_excel(writer, sheet_name='emb_cosine_distance'   )\n",
    "            df_emb_l2.to_excel(    writer, sheet_name='emb_euclidean_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<EOL>    0.948\n",
       "big      1.108\n",
       "happy    1.326\n",
       "dog      1.401\n",
       "good     1.463\n",
       "Name: <SOL>, dtype: float32"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = EmbeddingDiagnostics(model, train_dl)\n",
    "#d.get_word_input_counts()\n",
    "#d.get_word_cooccurences()\n",
    "#d.get_similar_n_words('<SOL>',direction = 'closest', metric='euclidean')\n",
    "#d.write2file(path_results)\n",
    "#d.plot_umap(n_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1541)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.get_baseline_loss(baseline='coocurrences')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
