{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used to write this: \n",
    "* https://github.com/bollu/bollu.github.io#everything-you-know-about-word2vec-is-wrong\n",
    "* https://www.youtube.com/watch?v=kEMJRjEdNzM&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe35112d2f0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torchtest\n",
    "import pandas as pd, numpy as np, datetime, scipy.misc\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from IPython.core.debugger import set_trace\n",
    "import plotly_express as px\n",
    "import umap \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "path_results = './results/word2vec/'\n",
    "path_runs = path_results + 'runs/'\n",
    "path_data = './data/'\n",
    "torch.manual_seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some thoughts on where to go from here: \n",
    "* ~~revamp data with the Dataset and DataLoader architecture~~ \n",
    "* ~~put algorithm into a class that calls torch.nn~~\n",
    "* ~~simplify code into functions~~\n",
    "* ~~shuffle sentences on the training set~~ \n",
    "* refactor out the loss function\n",
    "* establish input-independent baseline \n",
    "* visualise inputs before they go through model\n",
    "* ~~(No test set required here)~~ \n",
    "* work out how to make this algorithm in batches \n",
    "* adjust scaling of the starting parameters\n",
    "* ~~do an average loss function over a few batches rather than one~~\n",
    "* try on a real dataset\n",
    "* GloVe\n",
    "* something to handle unknown words, or words outside the top n \n",
    "\n",
    "Tensorboard\n",
    "* ~~hook it up to TensorBoard and print out some training graphs in real time~~\n",
    "* ~~visualise the gradients and the weights of the model~~\n",
    "* plot performance on a holdout set over time (??)\n",
    "* ~~log hyperparameters (m,lr, batch_sz)~~\n",
    "* ~~plot embeddings in tensorboard~~\n",
    "\n",
    "* profile the code: where are the bottlenecks?\n",
    "* ~~put this on github~~\n",
    "* intuitively understand the softmax part of this. what's the motivation, what are strengths and weaknesses going to be\n",
    "* try out Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "code_folding": [
     8,
     17,
     22,
     27,
     38,
     46
    ]
   },
   "outputs": [],
   "source": [
    "class SentenceData(Dataset): \n",
    "    def __init__(self, m): \n",
    "        self.m = m \n",
    "        self.corpus = self._load_train()\n",
    "        self.X = self._sen2tkn(self.corpus)\n",
    "        self.words,self.n_words  = list(set(self.X)),len(list(set(self.X)))\n",
    "        self.word2idx,self.idx2word = self._get_word_idx_mappings()\n",
    "\n",
    "    def _sen2tkn(self, l):\n",
    "        \"\"\"Convert list of sentences l into a concatenated list of tokens, adding <SOL> \n",
    "        and <EOL> tokens as needed.\"\"\"\n",
    "        def add_line_tokens(x):  return '<SOL> ' + x + ' <EOL>'\n",
    "        X_l = [add_line_tokens(o) + ' ' if i != (len(l)-1) \n",
    "                        else add_line_tokens(o) for i,o in enumerate(l)]\n",
    "        X = ''.join(X_l).split(' ')\n",
    "        return X\n",
    "\n",
    "    def _load_train(self): \n",
    "        ## read sentences, return list with one sentence per line\n",
    "        corpus = open(path_data + 'simple_sentences.txt').read().split('\\n')\n",
    "        return corpus\n",
    "\n",
    "    def _get_word_idx_mappings(self): \n",
    "        word2idx,idx2word = dict(),dict()\n",
    "        for i,o in enumerate(self.words): word2idx[o] = i; idx2word[i] = o\n",
    "        return word2idx,idx2word\n",
    "    \n",
    "    def _get_labels(self, x): \n",
    "        \"\"\"Loop through center words, get idx of context words. \n",
    "        Depends on m. \"\"\"\n",
    "        l = []\n",
    "        for i,c in enumerate(x): \n",
    "            min_idx,max_idx = max(0,i-self.m), min(len(x)-1,i+self.m)\n",
    "            for j in range(min_idx, max_idx + 1): \n",
    "                if j == i: continue \n",
    "                l.append(x[j])\n",
    "        return l \n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        if type(idx) is not slice:     idx = slice(idx,idx+1)\n",
    "        tmp = self.corpus[idx]\n",
    "        # convert to index \n",
    "        x = [self.word2idx[o] for o in self._sen2tkn(tmp)]\n",
    "        y = self._get_labels(x)\n",
    "        return (x,y)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss function here doesn't need labelled. \n",
    "* labels are implicit in the training code. We are updating U and V to match the training dataset. \n",
    "* this is more like unsupervised learning, where you don't have any labels, and your loss function will depend on things like inter/intra cluster distance, or here a likelihood over words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "m = 2\n",
    "train_ds = SentenceData(m=m)\n",
    "def collate_fn(x):\n",
    "    \"\"\"Flatten out list of lists\"\"\"\n",
    "    l = []\n",
    "    for o in x: l += o\n",
    "    return l\n",
    "# Batch size has become how many sentences to include in one row \n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "code_folding": [
     3,
     11,
     14
    ]
   },
   "outputs": [],
   "source": [
    "# For now, we do the preprocessing in the SentenceData class, \n",
    "# and we assume that processed data is passed to this class. \n",
    "class Word2Vec(nn.Module): \n",
    "    def __init__(self, m, emb_sz, ds):\n",
    "        \"\"\"If a layer has trainable parameters, it is defined in init. \n",
    "        ds = dataset\"\"\"\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.m,self.emb_sz,self.ds = m,emb_sz,ds\n",
    "        # U holds context words, V holds center words\n",
    "        self.U,self.V = self._init_embedding(),self._init_embedding()  \n",
    "        \n",
    "    def _init_embedding(self): \n",
    "        return nn.Parameter(torch.rand((self.emb_sz, self.ds.n_words))*1)\n",
    "    \n",
    "    def _softmax(self, x): \n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        z = x - max(x)\n",
    "        return torch.exp(z) / torch.sum(torch.exp(z))\n",
    "    \n",
    "    # Goal: get forward to return logits, or probabilities \n",
    "    # then use a separate loss function to evaluate those logits \n",
    "    # against some labels. \n",
    "    \n",
    "    def forward(self, x): \n",
    "        \"\"\" Returns list of probabilities\n",
    "        x is a tensor of some sort\"\"\" \n",
    "    #    loss_sum = 0\n",
    "   #     z = 0  # counts how many losses we add up (required for loss_sum)\n",
    "        p_l = []\n",
    "        for i, c in enumerate(x):   # enumerates for center word c \n",
    "            dotprods = self._get_dotprods(c) # use in calculating p below\n",
    "            min_idx,max_idx = max(0,i-self.m), min(len(x)-1,i+self.m)\n",
    "            for j in range(min_idx, max_idx+1):   # enumerate context words \n",
    "                if j != i:    # don't eval the center word\n",
    "                    log_p = self.predict(c, o=x[j], dotprods=dotprods)\n",
    "                    p_l.append(log_p)\n",
    "                 #   z += 1\n",
    "                 #   loss_sum += torch.log(p)\n",
    "       # loss_sum = -loss_sum / z\n",
    "       # return loss_sum\n",
    "        return torch.tensor(p_l)\n",
    "    \n",
    "    def _get_dotprods(self, c): \n",
    "        v,U = self.V[:,c].reshape(-1,1),self.U\n",
    "        dotprods = (v * U).sum(0)\n",
    "        return dotprods\n",
    "        \n",
    "    \n",
    "    def predict(self, c, o, dotprods=None):\n",
    "        \"\"\"\n",
    "        Takes current values of U and V and predicts the probability of context word o \n",
    "        appearing in context window of center word c \n",
    "        c: index of center word \n",
    "        o: index of context word\n",
    "        dotprods: saves computation if you pass those in\"\"\"\n",
    "        if dotprods is None: dotprods = self._get_dotprods(c)\n",
    "        return F.log_softmax(dotprods, dim=0)[o]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def loss_fn(log_p_list):    return -torch.mean(log_p)\n",
    "\n",
    "def add_run_info_to_tensorboard(hparam_d, metric_d, model): \n",
    "    writer.add_hparams(hparam_d, metric_d)\n",
    "    # Umap embeddings\n",
    "    emb = ((model.V+model.U) /2).T\n",
    "    writer.add_embedding(emb,metadata=model.ds.words,\n",
    "                     tag='final_embeddings')\n",
    "\n",
    "def update_tensorboard(global_step, loss, model, predict_sentences=False):\n",
    "    # loss, weights, grads\n",
    "    writer.add_scalar(tag='training_loss', scalar_value=loss, \n",
    "                          global_step=global_step)\n",
    "    writer.add_histogram(tag='V_weights', values=model.V, global_step=global_step)\n",
    "    writer.add_histogram(tag='U_weights', values=model.U, global_step=global_step)\n",
    "    writer.add_histogram(tag='V_grad', values=model.V.grad, global_step=global_step)\n",
    "    writer.add_histogram(tag='U_grad', values=model.U.grad, global_step=global_step)\n",
    "    if predict_sentences:\n",
    "        word_pairs = [('dog','cute'), ('cute', 'dog'), ('dog','dog'), ('<EOL>','<SOL>'),\n",
    "                      ('bank','money'), ('withdraw','money'), ('money','withdraw'),\n",
    "                      ('dog','money'), ('bank','cat')]\n",
    "        pred_d = dict()\n",
    "        for c, o in word_pairs: \n",
    "            key = \"p_\" + c + \"_\" + o\n",
    "            c_idx,o_idx = model.ds.word2idx[c],model.ds.word2idx[o]\n",
    "            pred_d[key] = torch.exp(model.predict(c_idx, o_idx)).item()\n",
    "        writer.add_scalars('example_predictions', pred_d, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0907, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1,w2='dog','cute'\n",
    "torch.exp(model.predict(model.ds.word2idx[w1], model.ds.word2idx[w2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [('dog','cute'), ('cute', 'dog'), ('dog','dog'), ('<EOL>','<SOL>'),\n",
    "                      ('bank','money'), ('withdraw','money'), ('money','withdraw'),\n",
    "                      ('dog','money'), ('bank','cat')]\n",
    "pred_d = dict()\n",
    "for w1, w2 in word_pairs: \n",
    "    key = \"p_\" + w1 + \"_\" + w2\n",
    "    pred_d[key] = torch.exp(model.predict(model.ds.word2idx[w1], \n",
    "                                          model.ds.word2idx[w2])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Run model \n",
    "m = 2  # number of context words\n",
    "emb_sz = 15  # number of embedding dimensions\n",
    "model = Word2Vec(m, emb_sz, train_ds)\n",
    "p_l = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-385-904c4269949c>\u001b[0m(31)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     29 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# don't eval the center word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m                \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 31 \u001b[0;31m                \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotprods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m                \u001b[0mz\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     33 \u001b[0;31m                \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> dotprods.size()\n",
      "torch.Size([16])\n",
      "ipdb> dotprods\n",
      "tensor([4.6152, 4.2370, 5.5688, 4.0938, 5.9525, 3.7064, 5.3563, 4.9338, 5.9124,\n",
      "        5.4910, 4.5994, 3.4754, 3.9856, 3.1876, 5.2777, 5.8819],\n",
      "       grad_fn=<SumBackward1>)\n",
      "ipdb> self._softmax(dotprods)\n",
      "tensor([0.0382, 0.0262, 0.0991, 0.0227, 0.1455, 0.0154, 0.0801, 0.0525, 0.1397,\n",
      "        0.0917, 0.0376, 0.0122, 0.0203, 0.0092, 0.0741, 0.1355],\n",
      "       grad_fn=<DivBackward0>)\n",
      "ipdb> np.sum(self._softmax(dotprods))\n",
      "*** TypeError: sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n",
      " * (torch.dtype dtype)\n",
      " * (tuple of names dim, bool keepdim, torch.dtype dtype)\n",
      " * (tuple of ints dim, bool keepdim, torch.dtype dtype)\n",
      "ipdb> torch.sum(self._softmax(dotprods))\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-386-7c08fcad9ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mthisloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss_movingavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthisloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-385-904c4269949c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# don't eval the center word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotprods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mz\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-385-904c4269949c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# don't eval the center word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotprods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mz\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## the word2vec algorithm with SGD \n",
    "# with SGDa\n",
    "lr = 0.01\n",
    "n_epochs = 400\n",
    "printfreq = 50\n",
    "batch_sz = 16\n",
    "idx = 0 \n",
    "epoch = 0 \n",
    "log_dir = path_runs + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = SummaryWriter(log_dir = log_dir)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_movingavg = []     # list to work out moving average of loss\n",
    "for epoch in range(n_epochs): \n",
    "    thisloss = 0\n",
    "    for i,(x,y) in enumerate(train_dl): \n",
    "        opt.zero_grad()\n",
    "        loss = model(x)\n",
    "        thisloss += loss.item() / len(train_dl)\n",
    "        loss_movingavg.append(thisloss)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if epoch % printfreq == 0:\n",
    "        global_step = epoch * len(train_dl)\n",
    "        loss = thisloss\n",
    "        update_tensorboard(global_step, loss, model, predict_sentences=True)\n",
    "        print(epoch, thisloss )\n",
    "        loss_movingavg = []\n",
    "\n",
    "# Hparams\n",
    "hparam_d = {'emb_sz': emb_sz, 'm': m, 'lr': lr, 'batch_sz':batch_sz,\n",
    "            'n_epochs':n_epochs, 'printfreq': printfreq}\n",
    "metric_d = {'hparam/loss': np.mean(loss_movingavg)}   \n",
    "add_run_info_to_tensorboard(hparam_d, metric_d, model)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure everything works as it is supposed to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "code_folding": [
     1,
     11,
     32,
     72,
     79,
     99,
     105,
     116,
     134
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingDiagnostics: \n",
    "    def __init__(self, model, dl, opt, seed=420): \n",
    "        \"\"\"dl: data_loader\"\"\"        \n",
    "        torch.manual_seed(seed)\n",
    "        self.model,self.dl,self.opt = model,dl,opt\n",
    "        # We average together V and U vectors to get the final embedding \n",
    "        self.emb = ((self.model.U + self.model.V) / 2).detach().numpy()\n",
    "        self.df_emb = pd.DataFrame(self.emb, columns =  self.model.ds.words)\n",
    "        self.df_emb_normalised = self._normalise_embedding_df(self.df_emb)\n",
    "             \n",
    "    def check_variables_update(self): \n",
    "        \"\"\"\n",
    "        This checks that parameters are being updated. \n",
    "        We run one forward pass+backward pass, and then update the parameters once, \n",
    "        and look at what changed. \n",
    "        \"\"\"\n",
    "        x = next(iter(self.dl))  # get one batch from data loader \n",
    "        params = [o for o in self.model.named_parameters() if o[1].requires_grad]\n",
    "        # copy initial values\n",
    "        initial_params = [(name, p.clone()) for (name, p) in params]\n",
    "        print(\"---- Parameters with 'requires_grad' and their sizes ------\")\n",
    "        for (name, p) in initial_params:  print(name, p.size())\n",
    "        # take a step\n",
    "        self.opt.zero_grad()\n",
    "        loss = self.model(x)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        print(\"---- Matrix norm of parameter update for one step ------\")\n",
    "        for (_,old_p), (name, new_p) in zip(initial_params, params): \n",
    "            print (name, torch.norm(new_p - old_p).item())       \n",
    "    \n",
    "    def get_baseline_loss(self, baseline='random'): \n",
    "        \"\"\"\n",
    "        Returns baseline loss to compare our model against. \n",
    "        If we don't get better than these losses, our model isn't learning anything. \n",
    "        \n",
    "        We run through one pass of the data, assuming that we predict each\n",
    "               word with equal probability. \n",
    "        Params\n",
    "            baseline: \n",
    "                \"random\": p(word) = 1/n_words, i.e. a uniform dist\n",
    "                \"bag_of_words\": p(word) = p(word|bag of words(train_set)) \n",
    "                \"coocurrences\": p(word) = p(word|center word c, distance m, train set)\n",
    "                                  Basically looking at word cofrequencies to get probability. \n",
    "        \"\"\"\n",
    "        z = 0  \n",
    "        loss_sum = 0\n",
    "        if baseline == \"bag_of_words\": \n",
    "            freqs = self.get_word_input_counts() / sum(self.get_word_input_counts())\n",
    "            freqs.index = [self.model.ds.word2idx[o] for o in freqs.index]\n",
    "            idx2freq = dict(freqs)\n",
    "        if baseline == 'coocurrences':\n",
    "            counts = self.get_word_cooccurences()\n",
    "            prob_mat = (counts / counts.values.sum(axis=1))\n",
    "        for x in self.dl:  # enumerate each example\n",
    "            for i, c in enumerate(x):  # enumerate center words \n",
    "                min_idx = max(0,          i - self.model.m)\n",
    "                max_idx = min(len(x) - 1, i + self.model.m)\n",
    "                for j in range(min_idx, max_idx + 1):   # enumerate context words \n",
    "                    if j == i: continue   # don't eval the center word\n",
    "                    if baseline == \"random\":            p = 1 / model.ds.n_words\n",
    "                    elif baseline == \"bag_of_words\":    p = idx2freq[x[j]]\n",
    "                    elif baseline == \"coocurrences\":\n",
    "                        p = prob_mat.iloc[:,c].loc[self.model.ds.idx2word[x[j]]]\n",
    "                    else:\n",
    "                        raise Exception('Baseline keyword not recognised')\n",
    "                    z += 1\n",
    "                    loss_sum += torch.log(torch.tensor(p))\n",
    "        loss_sum = -loss_sum / z\n",
    "        return loss_sum\n",
    "\n",
    "    def _normalise_embedding_df(self, df):  \n",
    "        return df /np.linalg.norm(df.values,ord=2,axis=0,keepdims=True)\n",
    "    \n",
    "    def get_word_input_counts(self):   \n",
    "        \"\"\"Count how often each word appears in input\"\"\"\n",
    "        return pd.Series(model.ds.X).value_counts()\n",
    "\n",
    "    def get_word_cooccurences(self): \n",
    "        \"\"\"\n",
    "        Count how often a word appears within distance m of another word \n",
    "        Returns a symmetrical data frame, where the column is the center word, \n",
    "         and the row is how often that word  appears in the m-window \n",
    "         around the center word\n",
    "        \"\"\" \n",
    "        m,n_words,X = self.model.m,self.model.ds.n_words,self.model.ds.X\n",
    "        words,word2idx = self.model.ds.words,self.model.ds.word2idx\n",
    "        count_df = pd.DataFrame(np.zeros((n_words,n_words)),dtype=int,\n",
    "                               index = words, columns = words)\n",
    "        # X is the list of words\n",
    "        for i,c in enumerate(X): \n",
    "            min_idx,max_idx = max(0,i-m),min(len(X)-1,i+m)\n",
    "            for j in range(min_idx,max_idx+1): \n",
    "                if j == i: continue\n",
    "                o_idx = word2idx[X[j]]\n",
    "                count_df.iloc[o_idx][word2idx[c]] += 1\n",
    "        return count_df\n",
    "    \n",
    "    def _get_dist_mat(self, df, metric): \n",
    "        \"\"\"df: pandas data frame; metric: anything fitting in pairwise distances\"\"\"\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        return pd.DataFrame(pairwise_distances(df.values.T, metric=metric).round(3), \n",
    "                           columns = self.df_emb.columns, index = self.df_emb.columns)\n",
    "    \n",
    "    def get_similar_n_words(self, w, n=5, direction='closest', metric='cosine'): \n",
    "        \"\"\"\n",
    "        Given a word w, what word is closest/furthest to it? \n",
    "            w: word, emb: embeddings as np array\n",
    "            direction: one of closest or furthest\"\"\"\n",
    "        dist_mat = self._get_dist_mat(self.df_emb, metric=metric)\n",
    "        v = dist_mat[w]\n",
    "        if    direction == 'closest' : return v.nsmallest(n+1)[1:]\n",
    "        elif  direction == 'furthest': return v.nlargest(n)\n",
    "        else: raise Exception(\"direction must be one of 'closest' or 'furthest'\")\n",
    "             \n",
    "    def plot_umap(self, n_dims=3, n_neighbours=3): \n",
    "        \"\"\"\n",
    "        Perform dim reduction with umap and plot results\n",
    "        n_dims: number of output dimensions \n",
    "        n_neighbours: local/global parameter for UMAP\"\"\"\n",
    "        if   n_dims == 2:   cols = ['dim1','dim2']\n",
    "        elif n_dims == 3:   cols = ['dim1','dim2','dim3']\n",
    "        else:               raise Exception('dims should be 2 or 3')\n",
    "        # put your data values in the fit_transform bit \n",
    "        emb_umap = umap.UMAP(n_neighbors=n_neighbours, n_components=n_dims).fit_transform(self.emb.T)\n",
    "        # put in pandas dataframe to help plotting with plotly express\n",
    "        emb_umap_df = pd.DataFrame(emb_umap, columns = cols)\n",
    "        emb_umap_df['word'] = self.model.ds.words\n",
    "        if n_dims == 2: \n",
    "            return px.scatter(emb_umap_df,    x='dim1', y='dim2',          hover_name='word')\n",
    "        elif n_dims ==3:\n",
    "            return px.scatter_3d(emb_umap_df, x='dim1', y='dim2',z='dim3', hover_name='word')\n",
    "\n",
    "    def write2file(self, path_results): \n",
    "        \"\"\"Write some distance matrices and embedding matrices to file to inspect manually\"\"\"\n",
    "        df_U = pd.DataFrame(self.model.U.detach().numpy())\n",
    "        df_V = pd.DataFrame(self.model.V.detach().numpy())\n",
    "        df_U.columns = df_V.columns = self.df_emb.columns\n",
    "\n",
    "        # distance matrices \n",
    "        df_emb_cosine = self._get_dist_mat(self.df_emb,'cosine')\n",
    "        df_emb_l2     = self._get_dist_mat(self.df_emb,'euclidean')\n",
    "        df_emb_cosine.columns = df_emb_l2.columns = self.df_emb.columns\n",
    "        df_emb_cosine.index   = df_emb_l2.index   = self.df_emb.columns\n",
    "\n",
    "        with pd.ExcelWriter(path_results + 'word2vec_vectors.xlsx') as writer: \n",
    "            df_V.to_excel(         writer, sheet_name='center_words'          )\n",
    "            df_U.to_excel(         writer, sheet_name='context_words'         )  \n",
    "            self.df_emb.to_excel(  writer, sheet_name='emb_words'             )\n",
    "            df_emb_cosine.to_excel(writer, sheet_name='emb_cosine_distance'   )\n",
    "            df_emb_l2.to_excel(    writer, sheet_name='emb_euclidean_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "d = EmbeddingDiagnostics(model, train_dl, opt)\n",
    "#d.get_word_input_counts()\n",
    "#d.get_word_cooccurences()\n",
    "#d.get_similar_n_words('<SOL>',direction = 'closest', metric='euclidean')\n",
    "#d.write2file(path_results)\n",
    "#d.plot_umap(n_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Parameters with 'requires_grad' and their sizes ------\n",
      "U torch.Size([15, 16])\n",
      "V torch.Size([15, 16])\n",
      "---- Matrix norm of parameter update for one step ------\n",
      "U 0.08234558999538422\n",
      "V 0.08205343037843704\n"
     ]
    }
   ],
   "source": [
    "#d.get_baseline_loss(baseline='coocurrences')\n",
    "d.check_variables_update()    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
