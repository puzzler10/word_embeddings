{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used to write this: \n",
    "* https://github.com/bollu/bollu.github.io#everything-you-know-about-word2vec-is-wrong\n",
    "* https://www.youtube.com/watch?v=kEMJRjEdNzM&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd, numpy as np, datetime, scipy.misc\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from IPython.core.debugger import set_trace\n",
    "import plotly_express as px\n",
    "import umap \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "path_results = './results/word2vec/'\n",
    "path_runs = path_results + 'runs/'\n",
    "path_data = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some thoughts on where to go from here: \n",
    "* ~~revamp data with the Dataset and DataLoader architecture~~ \n",
    "* put algorithm into a class that calls torch.nn\n",
    "* ~~simplify code into functions~~\n",
    "* ~~shuffle sentences on the training set~~ \n",
    "* work out how to make this algorithm in batches \n",
    "* adjust scaling of the starting parameters\n",
    "* ~~do an average loss function over a few batches rather than one~~\n",
    "\n",
    "Tensorboard\n",
    "* ~~hook it up to TensorBoard and print out some training graphs in real time~~\n",
    "* ~~visualise the gradients and the weights of the model~~\n",
    "* plot performance on a holdout set over time (??)\n",
    "* ~~log hyperparameters (m,lr, batch_sz)~~\n",
    "* ~~plot embeddings in tensorboard~~\n",
    "\n",
    "* profile the code: where are the bottlenecks?\n",
    "* ~~put this on github~~\n",
    "* manually calculate the gradients with finite differences and compare them to the autograd version\n",
    "* intuitively understand the softmax part of this. what's the motivation, what are strengths and weaknesses going to be\n",
    "* revamp using nn.Module \n",
    "* try out Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(420)\n",
    "emb_sz = 3  # number of embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "code_folding": [
     0,
     1,
     8,
     17,
     24,
     29,
     35
    ]
   },
   "outputs": [],
   "source": [
    "class SentenceData(Dataset): \n",
    "    def __init__(self, emb_sz): \n",
    "        self.emb_sz = emb_sz\n",
    "        self.corpus,self.corpus_test = self._load_train(),self._load_test()\n",
    "        self.X,self.X_test = self._sen2tkn(self.corpus),self._sen2tkn(self.corpus_test)\n",
    "        self.words,self.n_words  = list(set(self.X)),len(list(set(self.X)))\n",
    "        self.word2idx,self.idx2word = self._get_word_idx_mappings()\n",
    "\n",
    "    def _sen2tkn(self, l):\n",
    "        \"\"\"Convert list of sentences l into a concatenated list of tokens, adding <SOL> \n",
    "        and <EOL> tokens as needed.\"\"\"\n",
    "        def add_line_tokens(x):  return '<SOL> ' + x + ' <EOL>'\n",
    "        X_l = [add_line_tokens(o) + ' ' if i != (len(l)-1) \n",
    "                        else add_line_tokens(o) for i,o in enumerate(l)]\n",
    "        X = ''.join(X_l).split(' ')\n",
    "        return X\n",
    "\n",
    "    def _load_train(self): \n",
    "        ## read sentences, return list with one sentence per line\n",
    "        corpus = open(path_data + 'simple_sentences.txt').read().split('\\n')\n",
    "        return corpus\n",
    "    \n",
    "\n",
    "    \n",
    "    def _get_word_idx_mappings(self): \n",
    "        word2idx,idx2word = dict(),dict()\n",
    "        for i,o in enumerate(self.words): word2idx[o] = i; idx2word[i] = o\n",
    "        return word2idx,idx2word\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        if type(idx) is not slice:     idx = slice(idx,idx+1)\n",
    "        tmp = self.corpus[idx]\n",
    "        # convert to index \n",
    "        return [self.word2idx[o] for o in self._sen2tkn(tmp)]\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.corpus)\n",
    "    \n",
    "    def _load_test(self): \n",
    "        corpus_test = \"\"\"fluffy cute dog happy\n",
    "big happy dog cute\n",
    "small fluffy cat good fun\n",
    "bank good bank big\n",
    "withdraw money bank good\n",
    "deposit bank money\"\"\".split('\\n')\n",
    "        return corpus_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you want is for shuffle to shuffle sentences, but leave word order unchanged within a sentence. So you want shuffle to work on corpus, and not X. You want the indices to refer to words from the shuffled batch, I guess, since we are concatenating all the sentences together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SentenceData(emb_sz)\n",
    "def collate_fn(x):\n",
    "    \"\"\"Flatten out list of lists\"\"\"\n",
    "    l = []\n",
    "    for o in x: l += o\n",
    "    return l\n",
    "# Batch size has become how many sentences to include in one row \n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,o in enumerate(train_dl): \n",
    "    if i ==3: x=o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "code_folding": [
     3,
     11,
     14,
     19,
     37
    ]
   },
   "outputs": [],
   "source": [
    "# For now, we do the preprocessing in the SentenceData class, \n",
    "# and we assume that processed data is passed to this class. \n",
    "class Word2Vec(nn.Module): \n",
    "    def __init__(self, m, emb_sz, ds):\n",
    "        \"\"\"If a layer has trainable parameters, it is defined in init. \n",
    "        ds = dataset\"\"\"\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.m,self.emb_sz,self.ds = m,emb_sz,ds\n",
    "        # U holds context words, V holds center words\n",
    "        self.U,self.V = self._init_embedding(),self._init_embedding()  \n",
    "        \n",
    "    def _init_embedding(self): \n",
    "        return nn.Parameter(torch.rand((self.emb_sz, self.ds.n_words))*1)\n",
    "    \n",
    "    def _softmax(self, x): \n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        z = x - max(x)\n",
    "        return torch.exp(z) / torch.sum(torch.exp(z))\n",
    "    \n",
    "    def forward(self, x): \n",
    "        \"\"\"x is a tensor of some sort\"\"\" \n",
    "        loss_sum = 0\n",
    "        for i, c in enumerate(x):   # enumerates for center word c \n",
    "            # use in calculating p below\n",
    "            dotprods = (self.V[:,c].reshape(-1,1)*self.U).sum(0)  \n",
    "            min_idx,max_idx = max(0,i-self.m), min(len(x)-1,i+self.m)\n",
    "            for j in range(min_idx, max_idx+1):   # enumerate context words \n",
    "                if j == i: continue   # don't eval the center word\n",
    "                p = self._softmax(dotprods)[x[j]]\n",
    "                loss_sum += torch.log(p)\n",
    "        # last term below compensates for sliding window hitting the end of batch. \n",
    "        # it doesn't matter much.\n",
    "        # batch_sz*2*m <--> number of center words * number of context words\n",
    "        # sum(range(m,0,-1))*2 <--> adjust for end words \n",
    "        loss_sum = -loss_sum / (batch_sz*2*m - sum(range(m,0,-1))*2) \n",
    "        return loss_sum\n",
    "    \n",
    "    def predict(self, c, o):\n",
    "        \"\"\"Takes current values of U and V and predicts the probability of context word o \n",
    "        appearing in context window of center word c \"\"\"\n",
    "        c_idx,o_idx = self.ds.word2idx[c],self.ds.word2idx[o]\n",
    "        v = self.V[:,c_idx].reshape(-1,1)\n",
    "        dotprods = (v*self.U).sum(0)\n",
    "        return self._softmax(dotprods)[o_idx].item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def add_run_info_to_tensorboard(hparam_d, metric_d, model): \n",
    "    writer.add_hparams(hparam_d, metric_d)\n",
    "    # Umap embeddings\n",
    "    emb = ((model.V+model.U) /2).T\n",
    "    writer.add_embedding(emb,metadata=model.ds.words,\n",
    "                     tag='final_embeddings')\n",
    "\n",
    "def update_tensorboard(global_step, loss, model, predict_sentences=False):\n",
    "    # loss, weights, grads\n",
    "    writer.add_scalar(tag='training_loss', scalar_value=loss, \n",
    "                          global_step=global_step)\n",
    "    writer.add_histogram(tag='V_weights', values=model.V, global_step=global_step)\n",
    "    writer.add_histogram(tag='U_weights', values=model.U, global_step=global_step)\n",
    "    writer.add_histogram(tag='V_grad', values=model.V.grad, global_step=global_step)\n",
    "    writer.add_histogram(tag='U_grad', values=model.U.grad, global_step=global_step)\n",
    "    if predict_sentences:\n",
    "        # some example predictions\n",
    "        preds_d = {\n",
    "                'p_dog_cute':          model.predict('dog','cute'),\n",
    "                'p_cute_dog':          model.predict('cute', 'dog'),\n",
    "                'p_dog_dog':           model.predict('dog','dog'),\n",
    "                'p_eol_sol':           model.predict('<EOL>','<SOL>'),\n",
    "                'p_bank_money':        model.predict('bank','money'),\n",
    "                'p_withdraw_money':    model.predict('withdraw','money'),\n",
    "                'p_money_withdraw':    model.predict('money','withdraw'),\n",
    "                \"p_dog_money\":         model.predict('dog','money'),\n",
    "                \"p_bank_cat\":          model.predict('bank','cat')\n",
    "            }\n",
    "        writer.add_scalars('example_predictions', preds_d, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5959683284163475\n",
      "5 1.4460309259593487\n",
      "10 1.4356357231736183\n",
      "15 1.4386391751468182\n",
      "20 1.4148786589503288\n",
      "25 1.4268550388514996\n",
      "30 1.4139582067728043\n",
      "35 1.4261057190597057\n",
      "40 1.4144185669720173\n",
      "45 1.4180114232003689\n",
      "50 1.4246969409286976\n",
      "55 1.407885730266571\n",
      "60 1.4177313819527626\n",
      "65 1.4271469712257385\n",
      "70 1.4182912148535252\n",
      "75 1.414852563291788\n",
      "80 1.413858037441969\n",
      "85 1.4156530760228634\n",
      "90 1.4223078079521656\n",
      "95 1.418634794652462\n",
      "100 1.4289653450250626\n",
      "105 1.4194122701883316\n",
      "110 1.412411816418171\n",
      "115 1.4112755618989468\n",
      "120 1.4291682876646519\n",
      "125 1.4030862376093864\n",
      "130 1.5149525590240955\n",
      "135 1.4302466958761215\n",
      "140 1.4165453165769577\n",
      "145 1.4234930872917175\n",
      "150 1.4210439585149288\n",
      "155 1.4387550204992294\n",
      "160 1.490160770714283\n",
      "165 1.4186476469039917\n",
      "170 1.4181873425841331\n",
      "175 1.4591817110776901\n",
      "180 1.4080413058400154\n",
      "185 1.4344502463936806\n",
      "190 1.4190863035619259\n",
      "195 1.414304543286562\n",
      "200 1.4156725592911243\n",
      "205 1.420191828161478\n",
      "210 1.4128944873809814\n",
      "215 1.4169982075691223\n",
      "220 1.415448434650898\n",
      "225 1.4132524244487286\n",
      "230 1.4207336530089378\n",
      "235 1.423119880259037\n",
      "240 1.4166032299399376\n",
      "245 1.4131267294287682\n",
      "250 1.41545719653368\n",
      "255 1.4120743051171303\n",
      "260 1.4104290623217821\n",
      "265 1.4076133500784636\n",
      "270 1.4066012762486935\n",
      "275 1.4130441062152386\n",
      "280 1.4113928265869617\n",
      "285 1.4223354756832123\n",
      "290 1.407747196033597\n",
      "295 1.4017216488718987\n",
      "300 1.4021444767713547\n",
      "305 1.4078697487711906\n",
      "310 1.4150395393371582\n",
      "315 1.4168559350073338\n",
      "320 1.4157660156488419\n",
      "325 1.4105476401746273\n",
      "330 1.4168241620063782\n",
      "335 1.4055308923125267\n",
      "340 1.4130962267518044\n",
      "345 1.408510860055685\n",
      "350 1.4165931306779385\n",
      "355 1.4214337207376957\n",
      "360 1.4079173132777214\n",
      "365 1.420669324696064\n",
      "370 1.4124951101839542\n",
      "375 1.4244312047958374\n",
      "380 1.4085822142660618\n",
      "385 1.4137524031102657\n",
      "390 1.4166815988719463\n",
      "395 1.4171584025025368\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_run_info_to_tensorboard() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-fd371294d1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             'n_epochs':n_epochs, 'printfreq': printfreq}\n\u001b[1;32m     34\u001b[0m \u001b[0mmetric_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'hparam/loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0madd_run_info_to_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add_run_info_to_tensorboard() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "m = 2  # number of context words\n",
    "model = Word2Vec(m, emb_sz, train_ds)\n",
    "model(x)\n",
    "\n",
    "## the word2vec algorithm with SGD \n",
    "# with SGDa\n",
    "lr = 0.1\n",
    "n_epochs = 400\n",
    "printfreq = 5\n",
    "batch_sz = 16\n",
    "idx = 0 \n",
    "epoch = 0 \n",
    "log_dir = path_runs + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = SummaryWriter(log_dir = log_dir)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "for epoch in range(n_epochs): \n",
    "    thisloss = 0\n",
    "    for i,x in enumerate(train_dl): \n",
    "        opt.zero_grad()\n",
    "        loss = model(x)\n",
    "        thisloss += loss.item() / len(train_dl)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    loss_l.append(thisloss)\n",
    "    if epoch % printfreq == 0:\n",
    "        global_step = epoch * len(train_dl)\n",
    "        loss = thisloss\n",
    "        update_tensorboard(global_step, loss, model, predict_sentences=True)\n",
    "        print(epoch, loss_l[epoch])\n",
    "\n",
    "# Hparams\n",
    "hparam_d = {'emb_sz': emb_sz, 'm': m, 'lr': lr, 'batch_sz':batch_sz,\n",
    "            'n_epochs':n_epochs, 'printfreq': printfreq}\n",
    "metric_d = {'hparam/loss': loss}   \n",
    "add_run_info_to_tensorboard(hparam_d, metric_d, model)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ce4bd4bde1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_idx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# while epoch < n_epochs: \n",
    "#     loss_sum = 0\n",
    "#     X_batch,new_idx = get_batch(X,batch_sz,idx)\n",
    "#     if new_idx <= idx: \n",
    "#         X = shuffle_X(X_l)\n",
    "#         epoch += 1\n",
    "#         printflag = True \n",
    "#     else: printflag = False\n",
    "#     idx = new_idx\n",
    "#     for i, c in enumerate(X_batch):   # enumerates for center word c \n",
    "#         dotprods = (V[:,word2idx[c]].reshape(-1,1)*U).sum(0)   # use in calculating p below\n",
    "#         min_idx,max_idx = max(0,i-m), min(len(X_batch)-1,i+m)\n",
    "#         for j in range(min_idx, max_idx+1):   # enumerate context words \n",
    "#             if j == i: continue   # don't eval the center word\n",
    "#             p = softmax(dotprods)[word2idx[X_batch[j]]] \n",
    "#             loss_sum += torch.log(p)\n",
    "#           #  print(\"center:\",c,\"| context:\",o, \" \", p.item(), torch.log(p).item())   \n",
    "        \n",
    "#     # last term below compensates for sliding window hitting the end of batch. \n",
    "#     # it doesn't matter much.\n",
    "#     # batch_sz*2*m <--> number of center words * number of context words\n",
    "#     # sum(range(m,0,-1))*2 <--> adjust for end words \n",
    "#     loss_sum = -loss_sum / (batch_sz*2*m - sum(range(m,0,-1))*2) \n",
    "#     loss_l.append(loss_sum.item())\n",
    "#     loss_sum.backward()\n",
    "#     #if epoch % printfreq == (printfreq-1) and printflag: \n",
    "#     if epoch % printfreq == 0 and printflag:\n",
    "#         global_step = epoch*len(X) + i\n",
    "#         loss = np.mean(loss_l)\n",
    "#         update_tensorboard()\n",
    "#         print(epoch, global_step, np.mean(loss_l))\n",
    "#         loss_l = []\n",
    "#     U.data = U.data - lr * U.grad\n",
    "#     V.data = V.data - lr * V.grad\n",
    "#     U.grad.data.zero_()\n",
    "#     V.grad.data.zero_()\n",
    "# add_run_info_to_tensorboard()\n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     33,
     44
    ]
   },
   "outputs": [],
   "source": [
    "## prepare vectors and accessor dictionaries\n",
    "# def get_batch(X, batch_sz, curr_idx=0):\n",
    "#     \"\"\"Allows for batch updating\n",
    "#     X: corpus list of words, batch_sz: batch size, curr_idx: a running index \n",
    "#     Returns tuple: X_batch (to run word2vec over), and new_idx (the next \n",
    "#     iteration of running index )\"\"\"\n",
    "#     idx = curr_idx \n",
    "#     new_idx = idx + batch_sz \n",
    "#     X_sz = len(X) - 1 \n",
    "#     if new_idx > X_sz: \n",
    "#         new_idx = (new_idx % X_sz) - 1\n",
    "#         X_batch = X[idx:] + X[0:new_idx] \n",
    "#     else: \n",
    "#         X_batch = X[idx:new_idx]\n",
    "#     return X_batch,new_idx\n",
    "\n",
    "# def shuffle_X(X_l):\n",
    "#     \"\"\"X_l: list of sentences of X\"\"\"\n",
    "#     idxs = np.random.choice(range(len(X_l)),size = len(X_l),replace=False)\n",
    "#     X_l_new = np.array(X_l)[idxs].tolist()\n",
    "#     # make sure every sentence (except last) ends in space\n",
    "#     X_l_new = [o + ' ' if o[-1] != ' ' else o for o in X_l_new]\n",
    "#     X_l_new[-1] = X_l_new[-1][:-1]\n",
    "#     return ''.join(X_l_new).split(' ')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "code_folding": [
     1,
     8
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingDiagnostics: \n",
    "    def __init__(self, model): \n",
    "        self.model = model\n",
    "        # We average together V and U vectors to get the final embedding \n",
    "        self.emb = ((self.model.U + self.model.V) / 2).detach().numpy()\n",
    "        self.df_emb = pd.DataFrame(self.emb, columns =  self.model.ds.words)\n",
    "        self.df_emb_normalised = self._normalise_embedding_df(self.df_emb)\n",
    "\n",
    "    def _normalise_embedding_df(self, df):  \n",
    "        return df /np.linalg.norm(df.values,ord=2,axis=0,keepdims=True)\n",
    "    \n",
    "    def get_word_input_counts(self):   \n",
    "        \"\"\"Count how often each word appears in input\"\"\"\n",
    "        return pd.Series(model.ds.X).value_counts()\n",
    "\n",
    "    def get_word_cooccurences(self): \n",
    "        \"\"\"\n",
    "        Count how often a word appears within distance m of another word \n",
    "        Returns a symmetrical data frame, where the column is the center word, \n",
    "         and the row is how often that word  appears in the m-window \n",
    "         around the center word\n",
    "        \"\"\" \n",
    "        m,n_words,X = self.model.m,self.model.ds.n_words,self.model.ds.X\n",
    "        words,word2idx = self.model.ds.words,self.model.ds.word2idx\n",
    "        count_df = pd.DataFrame(np.zeros((n_words,n_words)),dtype=int,\n",
    "                               index = words, columns = words)\n",
    "        # X is the list of words\n",
    "        for i,c in enumerate(X): \n",
    "            min_idx,max_idx = max(0,i-m),min(len(X)-1,i+m)\n",
    "            for j in range(min_idx,max_idx+1): \n",
    "                if j == i: continue\n",
    "                o_idx = word2idx[X[j]]\n",
    "                count_df.iloc[o_idx][word2idx[c]] += 1\n",
    "        return count_df\n",
    "    \n",
    "    def _get_dist_mat(self, df, metric): \n",
    "        \"\"\"df: pandas data frame; metric: anything fitting in pairwise distances\"\"\"\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        return pd.DataFrame(pairwise_distances(df.values.T, metric=metric).round(3), \n",
    "                           columns = self.df_emb.columns, index = self.df_emb.columns)\n",
    "    \n",
    "    def get_similar_n_words(self, w, n=5, direction='closest', metric='cosine'): \n",
    "        \"\"\"\n",
    "        Given a word w, what word is closest/furthest to it? \n",
    "            w: word, emb: embeddings as np array\n",
    "            direction: one of closest or furthest\"\"\"\n",
    "        dist_mat = self._get_dist_mat(self.df_emb, metric=metric)\n",
    "        v = dist_mat[w]\n",
    "        if    direction == 'closest' : return v.nsmallest(n+1)[1:]\n",
    "        elif  direction == 'furthest': return v.nlargest(n)\n",
    "        else: raise Exception(\"direction must be one of 'closest' or 'furthest'\")\n",
    "             \n",
    "    def plot_umap(self, n_dims=3, n_neighbours=3): \n",
    "        \"\"\"\n",
    "        Perform dim reduction with umap and plot results\n",
    "        n_dims: number of output dimensions \n",
    "        n_neighbours: local/global parameter for UMAP\"\"\"\n",
    "        if   n_dims == 2:   cols = ['dim1','dim2']\n",
    "        elif n_dims == 3:   cols = ['dim1','dim2','dim3']\n",
    "        else:               raise Exception('dims should be 2 or 3')\n",
    "        # put your data values in the fit_transform bit \n",
    "        emb_umap = umap.UMAP(n_neighbors=n_neighbours, n_components=n_dims).fit_transform(self.emb.T)\n",
    "        # put in pandas dataframe to help plotting with plotly express\n",
    "        emb_umap_df = pd.DataFrame(emb_umap, columns = cols)\n",
    "        emb_umap_df['word'] = self.model.ds.words\n",
    "        if n_dims == 2: \n",
    "            return px.scatter(emb_umap_df,    x='dim1', y='dim2',          hover_name='word')\n",
    "        elif n_dims ==3:\n",
    "            return px.scatter_3d(emb_umap_df, x='dim1', y='dim2',z='dim3', hover_name='word')\n",
    "\n",
    "    def write2file(self, path_results): \n",
    "        \"\"\"Write some distance matrices and embedding matrices to file to inspect manually\"\"\"\n",
    "        df_U = pd.DataFrame(self.model.U.detach().numpy())\n",
    "        df_V = pd.DataFrame(self.model.V.detach().numpy())\n",
    "        df_U.columns = df_V.columns = self.df_emb.columns\n",
    "\n",
    "        # distance matrices \n",
    "        df_emb_cosine = self._get_dist_mat(self.df_emb,'cosine')\n",
    "        df_emb_l2     = self._get_dist_mat(self.df_emb,'euclidean')\n",
    "        df_emb_cosine.columns = df_emb_l2.columns = self.df_emb.columns\n",
    "        df_emb_cosine.index   = df_emb_l2.index   = self.df_emb.columns\n",
    "\n",
    "        with pd.ExcelWriter(path_results + 'word2vec_vectors.xlsx') as writer: \n",
    "            df_V.to_excel(         writer, sheet_name='center_words'          )\n",
    "            df_U.to_excel(         writer, sheet_name='context_words'         )  \n",
    "            self.df_emb.to_excel(  writer, sheet_name='emb_words'             )\n",
    "            df_emb_cosine.to_excel(writer, sheet_name='emb_cosine_distance'   )\n",
    "            df_emb_l2.to_excel(    writer, sheet_name='emb_euclidean_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<EOL>    0.181\n",
       "good     0.278\n",
       "big      0.506\n",
       "money    1.005\n",
       "dog      1.201\n",
       "Name: <SOL>, dtype: float32"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = EmbeddingDiagnostics(model)\n",
    "d.get_word_input_counts()\n",
    "d.get_word_cooccurences()\n",
    "d.get_similar_n_words('<SOL>',direction = 'closest', metric='euclidean')\n",
    "d.write2file(path_results)\n",
    "d.plot_umap(n_dims=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
